<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="feed-forward-neural-networks" class="unnumbered">Feed Forward Neural Networks</h1>
<p>In this part of the tutorial I will go through the components of a fully connected feed forward neural network and show how gradient descent and backpropagation can be used to optimize the parameters of the neural network for a certain task. Our training data will be a collection of <span class="math inline">\(N\)</span> objects represented by input and output vectors <span class="math inline">\((\textbf{x}_1, \textbf{y}_1),...,(\textbf{x}_N, \textbf{y}_N)\)</span>. Each vector <span class="math inline">\(\textbf{x}_n\)</span> has length <span class="math inline">\(M\)</span>, where each entry of the vector represent a different feature (or measurement) of object <span class="math inline">\(n\)</span>. The network itself has trainable parameters <span class="math inline">\(\textbf{W}^l\)</span> and <span class="math inline">\(\textbf{b}^l\)</span>. The weights of the network are stored in the series of matrices <span class="math inline">\(\textbf{W}^l\)</span>, where the <span class="math inline">\(W_{ij}^l\)</span> entry of the tensor represents the weight of the connection from neuron <span class="math inline">\(j\)</span> in layer <span class="math inline">\(l-1\)</span> to neuron <span class="math inline">\(i\)</span> in layer <span class="math inline">\(l\)</span>. The series of vectors <span class="math inline">\(\textbf{b}^l\)</span> represent the offset the neurons in layer <span class="math inline">\(l\)</span>, where the entry <span class="math inline">\(b_{i}^l\)</span> is the offset of neuron <span class="math inline">\(i\)</span> in layer <span class="math inline">\(l\)</span>. The activations of and inputs to the neurons in the network can be represented with the following equations <span class="math display">\[\begin{split}
\textbf{a}^0 &amp;= \textbf{x} \\
\textbf{z}^l &amp;= \textbf{W}^l \textbf{a}^{l-1} + \textbf{b}^l \\
\textbf{a}^l &amp;= \sigma_l (\textbf{z}^l) \\
\hat{\textbf{y}} &amp;= \textbf{a}^{L},
\end{split}\]</span> where the subscript <span class="math inline">\(n\)</span>, which denotes the training example, is suppressed so as to make the equations more transparent. Here <span class="math inline">\(\textbf{z}^l\)</span> are the inputs to the neurons in layer <span class="math inline">\(l\)</span>. By applying a sigmoidal activation function <span class="math inline">\(\sigma_l\)</span> to the inputs <span class="math inline">\(\textbf{z}^l\)</span> the activations <span class="math inline">\(\textbf{a}^l\)</span> of the neurons in layer <span class="math inline">\(l\)</span> are obtained. In practice this activation function is usually a tanh, logistic, or relu function. This function can also be different for each layer of the network. For simplicity of notation the activations in the layer <span class="math inline">\(l=0\)</span> are the input vector <span class="math inline">\(\textbf{x}\)</span>, and the activations of the last layer <span class="math inline">\(l=L\)</span> are the predicted output <span class="math inline">\(\hat{\textbf{y}}\)</span>. In component form these equations are. <span class="math display">\[\label{eq:nn_components}
\begin{split}
a_j^0 &amp;= x_j \\
z_i^l &amp;= \sum_{j=0} W_{ij}^l a_j^{l-1} + b_i^l \\
a_i^l &amp;= \sigma_l (z_i^l) \\
\hat{y} &amp;= a_i^{L}
\end{split}\]</span></p>
<h1 id="cost-functions" class="unnumbered">Cost Functions</h1>
<p>The difference between the predicted output and the known output, for the <span class="math inline">\(n^{th}\)</span> training example, for a given set of parameters <span class="math inline">\(\textbf{W}^l\)</span> and <span class="math inline">\(\textbf{b}^l\)</span>, can be quantified by defining the loss function <span class="math inline">\(E_k(\textbf{x}_n, \textbf{y}_n ; \textbf{W}^l, \textbf{b}^l)\)</span>. For regression problems the cost function is often chosen to be the squared error loss <span class="math display">\[E_n = \frac{1}{2} \left |\hat{\textbf{y}}_n - \textbf{y}_n \right|^2\]</span> For classification problems with multiple classes the cross entropy loss is used. This requires the use of the softmax activation function in the last layer to ensure the output neurons of the network output probabilities between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. The softmax activation function is <span class="math display">\[\hat{\textbf{y}} = \sigma_L(\textbf{z}^{l-1}) = \frac{e^{\textbf{z}^{l-1}}}{\text{sum} \left( e^{\textbf{z}^{l-1}} \right)},\]</span> where <span class="math inline">\(\text{sum} \left( e^{\textbf{z}^{l-1}} \right)\)</span> is the sum of elements of the vector <span class="math inline">\(e^{\textbf{z}^{l-1}}\)</span>. The cross entropy loss of the <span class="math inline">\(n^{th}\)</span> training example is then given by the dot product between the true label vector <span class="math inline">\(\textbf{y}_n\)</span> and the the log of the predicted labels from the network <span class="math inline">\(\log{\hat{\textbf{y}}_n}\)</span> <span class="math display">\[E_n = - \textbf{y}_n \cdot \log{\hat{\textbf{y}}_n}.\]</span> The cost function for the entire training set is then the sum of the losses for each individual training example <span class="math display">\[E \left(\textbf{W}^l, \textbf{b}^l \right) = \frac{1}{N} \sum_{n=0}^N E_n(\textbf{x}_n, \textbf{y}_n ; \textbf{W}^l, \textbf{b}^l)\]</span></p>
<h2 id="minimizing-the-cost-function-with-backpropagation" class="unnumbered">Minimizing the Cost Function with Backpropagation</h2>
<p>Optimizing the network for a particular training set means minimizing the cost function <span class="math inline">\(E \left(\textbf{W}^l, \textbf{b}^l \right)\)</span> as a function of the weights and offsets parameters. In practice the number of trainable parameters can be very large so it is impractical to use brute force to minimize the cost function. Backpropagation is the traditional gradient descent algorithm combined with the use of the chain rule to calculate the derivatives of the cost function with respect to the training parameters. Gradient descent works off the observation that from any given point in parameter space the fastest way to get to a local minimum of the cost function is to travel in the negative direction of the gradient of the cost function at that point. Thus by updating the parameters according to the rules <span class="math display">\[\begin{split}
W^l_{ij} &amp; \rightarrow W^l_{ij} - \alpha \frac{\partial E}{\partial W^l_{ij}} \\
b^l_i &amp; \rightarrow b^l_{i} - \alpha \frac{\partial E}{\partial b^l_i} 
\end{split}\]</span> eventually the values of the trainable parameters will be such that the cost function is at a local minimum. The quantity <span class="math inline">\(\alpha\)</span> is called the learning rate. It should be tuned to a value that ensures the gradient descent algorithm reaches a local minimum in a reasonable amount of time.</p>
<p>From the above update rules it can be seen that gradient descent requires knowledge of the derivatives of the cost function with respect to the training parameters. In theory these derivatives can be calculated numerically, but this approach is inefficient and prone to numerical error. A better approach is to use the chain rule to write the derivatives as <span class="math display">\[\begin{split}
\frac{\partial E_n}{\partial W^l_{ij}} &amp;= \frac{\partial E_n}{\partial z_i^l } \frac{\partial z_i^l}{\partial W^l_{ij}} \\
\frac{\partial E_n}{\partial b^l_{ij}} &amp;= \frac{\partial E_n}{\partial z_i^l} \frac{\partial z_i^l}{\partial b^l_i}
\end{split}\]</span> From the definition of <span class="math inline">\(z_i^l\)</span> the inputs to the neurons in layers <span class="math inline">\(l\)</span> can be written <span class="math inline">\(z_i^l=\sum_{k=0} W_{ik}^l a_k^l + b_i^l\)</span>. Thus <span class="math display">\[\begin{split}
\frac{\partial z_i^l}{\partial W^l_{ij}} &amp;= \frac{\partial}{\partial W^l_{ij}} \left( \sum_{k=0} W_{ik}^l a_k^{l-1} + b_i^l \right) = a_j^{l-1} \\
\frac{\partial z_i^l}{\partial b^l_i} &amp;= \frac{\partial}{\partial b^l_{i}} \left( \sum_{k=0} W_{ik}^l a_k^{l-1} + b_i^l \right) = 1
\end{split}\]</span> Furthermore the derivative <span class="math inline">\(\frac{\partial E_n}{\partial z_i^l }\)</span> has the interpretation of being the “error” of the network in layer <span class="math inline">\(l\)</span>. This quantity is usually given the name <span class="math inline">\(\delta_i^l = \frac{\partial E_n}{\partial z_i^l }\)</span>. Inserting these expressions into the above equations, the derivatives of the cost function at layer <span class="math inline">\(l\)</span> can be written in terms of the activation at layer <span class="math inline">\(l-1\)</span> and the errors at layer <span class="math inline">\(l\)</span>. <span class="math display">\[\begin{split}
\frac{\partial E_n}{\partial W^l_{ij}} &amp;= \delta_i^l a_j^{l-1} \\
\frac{\partial E_n}{\partial b^l_{ij}} &amp;= \delta_i^l
\end{split}\]</span> Now all that’s left is to write the errors in terms of the activations using the chain rule. For a hidden layer this error can be written as <span class="math display">\[\delta_j^{l-1} = \frac{\partial E_n}{\partial z_j^{l-1} } = \sum_k \frac{\partial E_n}{\partial z_k^l } \frac{\partial z_k^l}{\partial z_j^{l-1} }\]</span> Using the definition of the inputs to the neurons in layer <span class="math inline">\(l\)</span> <span class="math display">\[\begin{split}
\frac{\partial z_k^l}{\partial z_j^{l-1}} &amp;= \frac{\partial }{\partial z_j^{l-1}} \left(\sum_k W_{ik}^l a_k^{l-1} + b_k^l \right) \\
&amp;= \frac{\partial }{\partial z_j^{l-1}} \left(\sum_k W_{ik}^l \sigma_l (z_k^{l-1}) + b_k^l \right) \\
&amp;=W_{ij}^l \sigma_l &#39;(z_j^{l-1}) 
\end{split}\]</span> Thus <span class="math display">\[\begin{split}
\delta_j^{l-1} 
&amp;= \sum_k \frac{\partial E_n}{\partial z_k^l } W_{kj}^l \sigma_l &#39;(z_j^{l-1}) \\
&amp;= \sigma_l &#39;(z_j^{l-1}) \sum_k \delta_k^l W_{kj}^l 
\end{split}\]</span> For the output layer <span class="math display">\[\delta_i^{L} 
= \frac{\partial E_n}{\partial z_i^{L}} 
= \frac{\partial E_n}{\partial \hat{y}_i} \frac{\partial \hat{y}_i}{\partial z_i^{L}}\]</span> For the squared error loss function (suppressing the <span class="math inline">\(n\)</span> subscript) this becomes <span class="math display">\[\delta_i^{L} 
= \frac{\partial}{\partial z_i^L} \left(\frac{1}{2} \left |\hat{\textbf{y}} - \textbf{y} \right|^2 \right) \frac{\partial \hat{y}_i}{\partial z_i^{L}}
= \left( \hat{y}_i - y_i \right) \sigma_L&#39;(z_i^L)\]</span> For the cross entropy loss this becomes <span class="math display">\[\delta_i^{L} 
= - \frac{\partial}{\partial \hat{y}_i} \left(\textbf{y} \cdot \log{\hat{\textbf{y}}} \right) \frac{\partial \hat{y}_i}{\partial z_i^{L}}
= -\frac{y_i}{\hat{y}_i} \sigma_L&#39;(z_i^L)\]</span></p>
</body>
</html>
